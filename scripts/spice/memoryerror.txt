2023-06-27 10:21:27.745832: W external/tsl/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.20GiB (rounded to 13094714880)requested by op 
2023-06-27 10:21:27.748167: W external/tsl/tsl/framework/bfc_allocator.cc:497] ********************________________________________________________________________________________
2023-06-27 10:21:27.760988: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2461] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 13094714656 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.42GiB
              constant allocation:        48B
        maybe_live_out allocation:    9.60MiB
     preallocated temp allocation:   12.20GiB
  preallocated temp fragmentation:   14.37MiB (0.12%)
                 total allocation:   13.62GiB
              total fragmentation:   14.45MiB (0.10%)
Peak buffers:
	Buffer 1:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 2:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 3:
		Size: 205.62MiB
		Entry Parameter Subshape: s32[898367,60]
		==========================

	Buffer 4:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 5:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 6:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 7:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 8:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 9:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 10:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d5/d5.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 11:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d4/d4.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 12:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d3/d3.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 13:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d2/d2.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 14:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d1/d1.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 15:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d0/d0.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================


Traceback (most recent call last):
  File "/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py", line 190, in <module>
    run(sys.argv[1])
  File "/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py", line 155, in run
    state = epoch(state, i_tr, x_tr, f_tr, y_tr)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/traceback_util.py", line 166, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/pjit.py", line 250, in cache_miss
    outs, out_flat, out_tree, args_flat, jaxpr = _python_pjit_helper(
                                                 ^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/pjit.py", line 163, in _python_pjit_helper
    out_flat = pjit_p.bind(*args_flat, **params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/core.py", line 2652, in bind
    return self.bind_with_trace(top_trace, args, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/core.py", line 383, in bind_with_trace
    out = trace.process_primitive(self, map(trace.full_raise, args), params)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/core.py", line 790, in process_primitive
    return primitive.impl(*tracers, **params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/pjit.py", line 1193, in _pjit_call_impl
    return xc._xla.pjit(name, f, call_impl_cache_miss, [], [], donated_argnums,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/pjit.py", line 1177, in call_impl_cache_miss
    out_flat, compiled = _pjit_call_impl_python(
                         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/pjit.py", line 1133, in _pjit_call_impl_python
    return compiled.unsafe_call(*args), compiled
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/profiler.py", line 314, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/naglea/miniconda3/envs/jax/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py", line 1347, in __call__
    results = self.xla_executable.execute_sharded(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jax._src.traceback_util.UnfilteredStackTrace: jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 13094714656 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.42GiB
              constant allocation:        48B
        maybe_live_out allocation:    9.60MiB
     preallocated temp allocation:   12.20GiB
  preallocated temp fragmentation:   14.37MiB (0.12%)
                 total allocation:   13.62GiB
              total fragmentation:   14.45MiB (0.10%)
Peak buffers:
	Buffer 1:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 2:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 3:
		Size: 205.62MiB
		Entry Parameter Subshape: s32[898367,60]
		==========================

	Buffer 4:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 5:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 6:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 7:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 8:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 9:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 10:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d5/d5.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 11:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d4/d4.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 12:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d3/d3.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 13:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d2/d2.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 14:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d1/d1.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 15:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d0/d0.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py", line 190, in <module>
    run(sys.argv[1])
  File "/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py", line 155, in run
    state = epoch(state, i_tr, x_tr, f_tr, y_tr)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
jaxlib.xla_extension.XlaRuntimeError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 13094714656 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:    1.42GiB
              constant allocation:        48B
        maybe_live_out allocation:    9.60MiB
     preallocated temp allocation:   12.20GiB
  preallocated temp fragmentation:   14.37MiB (0.12%)
                 total allocation:   13.62GiB
              total fragmentation:   14.45MiB (0.10%)
Peak buffers:
	Buffer 1:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 2:
		Size: 616.86MiB
		Entry Parameter Subshape: f32[898367,60,3]
		==========================

	Buffer 3:
		Size: 205.62MiB
		Entry Parameter Subshape: s32[898367,60]
		==========================

	Buffer 4:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 5:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 6:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/transpose(jvp(jit(get_e_pred)))/Model/model/d2/d2.spatial_attention/x_mixing/mul" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57 deduplicated_name="fusion.2141"
		XLA Label: fusion
		Shape: f32[32,60,60,256]
		==========================

	Buffer 7:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 8:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 9:
		Size: 112.50MiB
		XLA Label: fusion
		Shape: f32[115200,256]
		==========================

	Buffer 10:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d5/d5.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 11:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d4/d4.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 12:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d3/d3.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 13:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d2/d2.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 14:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d1/d1.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================

	Buffer 15:
		Size: 112.50MiB
		Operator: op_name="jit(epoch)/jit(main)/while/body/jit(step_with_loss)/jvp(jit(get_e_pred_sum))/jvp(jit(get_e_pred))/Model/model/d0/d0.spatial_attention/x_mixing/layers_0/dot_general[dimension_numbers=(((3,), (0,)), ((), ())) precision=None preferred_element_type=None]" source_file="/lila/data/chodera/naglea/sake/scripts/spice/run_batch_force_loss.py" source_line=57
		XLA Label: custom-call
		Shape: f32[115200,256]
		==========================
